# [\#84 Issue](https://github.com/OWASP/www-project-machine-learning-security-top-10/issues/84) `open`: [FEEDBACK]: Rename adversarial attack to something less ambiguous
**Labels**: `issues/general`, `issues/triage`


#### <img src="https://avatars.githubusercontent.com/u/796794?v=4" width="50">[robvanderveer](https://github.com/robvanderveer) opened issue at [2023-08-19 13:58](https://github.com/OWASP/www-project-machine-learning-security-top-10/issues/84):

### Type

Suggestions for Improvement

### What would you like to report?

The term adversarial attack usually has a broader definition than the intention of ML01. For example it usually includes data poisoning. 
The intention seems to refer to what is more often called 'evasion attack'. The problem with that term is that it usually means small changes to the input. This is why in the AI guide we used the term 'input manipulation', which is more clear.


### Code of Conduct

- [X] I agree to follow this project's Code of Conduct




-------------------------------------------------------------------------------



[Export of Github issue for [OWASP/www-project-machine-learning-security-top-10](https://github.com/OWASP/www-project-machine-learning-security-top-10).]
